{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9faaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EsFeatures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9db02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "EsFeatures['a'] = set(('low', 'front', 'lax', 'unrounded')) \n",
    "EsFeatures['e'] = set(('mid', 'front', 'tense', 'unrounded')) \n",
    "EsFeatures['i'] = set(('high', 'front', 'tense', 'unrounded')) \n",
    "EsFeatures['o'] = set(('mid', 'back', 'tense', 'rounded')) \n",
    "EsFeatures['u'] = set(('high', 'back', 'tense', 'rounded')) \n",
    "EsFeatures['ɛ'] = set(('mid', 'front', 'lax', 'unrounded')) \n",
    "EsFeatures['ɪ'] = set(('high', 'front', 'lax', 'unrounded')) \n",
    "EsFeatures['n'] = set(('nasal', 'alveolar', 'voiced'))\n",
    "EsFeatures['f'] = set(('fricative', 'labiodental', 'voiceless'))\n",
    "EsFeatures['m'] = set(('nasal', 'bilabial', 'voiced'))\n",
    "EsFeatures['b'] = set(('plosive', 'bilabial', 'voiced'))\n",
    "EsFeatures['ɾ'] = set(('tap', 'alveolar','voiced'))\n",
    "EsFeatures['k'] = set(('plosive', 'velar', 'voiceless'))\n",
    "EsFeatures['l'] = set(('lateral approximant', 'alveolar', 'voiced'))\n",
    "EsFeatures['d'] = set(('plosive','alveolar','voiced'))\n",
    "EsFeatures['v'] = set(('fricative', 'labiodental','voiceless'))\n",
    "EsFeatures['p'] = set(('plosive', 'bilabial','voiceless'))\n",
    "EsFeatures['j'] = set(('approximant', 'palatal','voiced'))\n",
    "EsFeatures['ð'] = set(('fricative', 'dental','voiced'))\n",
    "EsFeatures['w'] = set(('approximant','bilabial','voiced'))\n",
    "EsFeatures['ʃ'] = set(('fricative', 'post-alveolar','voiceless'))\n",
    "EsFeatures['t'] = set(('plosive', 'alveolar', 'voiceless'))\n",
    "EsFeatures['ɡ'] = set(('plosive', 'velar', 'voiced'))\n",
    "EsFeatures['ŋ'] = set(('nasal', 'velar','voiced'))\n",
    "EsFeatures['s'] = set(('fricative','alveolar', 'voiceless'))\n",
    "EsFeatures['d͡ʒ'] = set(('affricate', 'post-alveolar', 'voiced'))\n",
    "EsFeatures['t͡ʃ'] = set(('affricate', 'post-alveolar', 'voiceless'))\n",
    "EsFeatures['β'] = set(('fricative' 'bilabial', 'voiced'))\n",
    "EsFeatures['x'] = set(('fricative', 'velar', 'voiceless'))\n",
    "EsFeatures['ɣ'] = set(('fricative', 'velar', 'voiced'))\n",
    "EsFeatures['ʝ'] = set(('fricative', 'palatal', 'voiced'))\n",
    "EsFeatures['ʎ'] = set(('lateral approximant', 'palatal', 'voiced'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c028b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "esvowels = set(('a', 'e', 'i', 'o', 'u', 'ɛ', 'ɪ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d11a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "escons = set(('n', 'f', 'm', 'b', 'ɾ', 'k','l', 'd', 'v', 'p', 'j', 'ð', 'w', 'ʃ', 't', 'ɡ', 'ŋ', 's', 'd͡ʒ', 't͡ʃ', 'β', 'x', 'ɣ', 'ʝ', 'ʎ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41c8d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def matrix(token1, token2): \n",
    "    \n",
    "    distances = numpy.zeros((len(token1) + 1, len(token2) + 1)) \n",
    "    \n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2   \n",
    "    \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    return distances\n",
    "    \n",
    "\n",
    "def penalty(t1, t2, distances, penalty): \n",
    "    \n",
    "    a = distances[t1][t2 - 1]\n",
    "    b = distances[t1 - 1][t2]\n",
    "    c = distances[t1 - 1][t2 - 1]\n",
    "    \n",
    "    if (a <= b and a <= c):\n",
    "        distances[t1][t2] = a + penalty\n",
    "    elif (b <= a and b <= c):\n",
    "        distances[t1][t2] = b + penalty\n",
    "    else:\n",
    "        distances[t1][t2] = c + penalty\n",
    "    \n",
    "    return distances \n",
    "\n",
    "def OldLD(token1, token2): \n",
    "    \n",
    "    distances = matrix(token1, token2)\n",
    "\n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):   \n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1-1][t2-1]\n",
    "            else:\n",
    "                if token1[t1-1] in escons: \n",
    "                    if token2[t2-1] in escons:\n",
    "                        if len(EsFeatures[token1[t1-1]]-EsFeatures[token2[t2-1]]) == 1: \n",
    "                            distances = penalty(t1, t2, distances, .3)\n",
    "                        elif len(EsFeatures[token1[t1-1]]-EsFeatures[token2[t2-1]]) == 2:\n",
    "                            distances = penalty(t1, t2, distances, .6)\n",
    "                        else: \n",
    "                            distances = penalty(t1, t2, distances, 1)\n",
    "                    else:\n",
    "                        distances = penalty(t1, t2, distances, 1) \n",
    "                elif token1[t1-1] in esvowels: \n",
    "                    if token2[t2-1] in esvowels: \n",
    "                        if len(EsFeatures[token1[t1-1]]-EsFeatures[token2[t2-1]]) == 1: \n",
    "                            distances = penalty(t1, t2, distances, .25)\n",
    "                        elif len(EsFeatures[token1[t1-1]]-EsFeatures[token2[t2-1]]) == 2:\n",
    "                            distances = penalty(t1, t2, distances, .5)\n",
    "                        elif len(EsFeatures[token1[t1-1]]-EsFeatures[token2[t2-1]]) == 3:\n",
    "                            distances = penalty(t1, t2, distances, .75)\n",
    "                        else: \n",
    "                            distances = penalty(t1, t2, distances, 1) \n",
    "                    else: \n",
    "                        distances = penalty(t1, t2, distances, 1) \n",
    "                else:\n",
    "                    distances = penalty(t1, t2, distances, 1)  \n",
    "                    \n",
    "    return distances, distances[len(token1)][len(token2)]\n",
    "\n",
    "         \n",
    "    \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47e279d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0. , 1. , 2. , 3. , 4. , 5. , 6. ],\n",
       "        [1. , 0.6, 1.6, 1.9, 2.5, 3.5, 3.8],\n",
       "        [2. , 1.6, 0.6, 1.6, 2.6, 3. , 4. ],\n",
       "        [3. , 2.6, 1.6, 1.2, 1.6, 2.6, 3.2],\n",
       "        [4. , 3.6, 2.1, 2.2, 2.2, 1.6, 2.6]]),\n",
       " 2.6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OldLD('deðo', 'peɾðon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f3f9ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0. , 1. , 2. , 3. , 4. ],\n",
       "        [1. , 0.3, 1.3, 2. , 3. ],\n",
       "        [2. , 1.3, 0.3, 1.3, 2.3],\n",
       "        [3. , 1.6, 1.3, 0.3, 1.3]]),\n",
       " 1.3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OldLD('pip', 'bipp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4aebba31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 2., 3., 4.],\n",
       "        [1., 1., 2., 3., 4.],\n",
       "        [2., 2., 2., 3., 4.],\n",
       "        [3., 3., 3., 3., 3.]]),\n",
       " 3.0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OldLD('abc', 'xyzc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6fd240a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 2., 3., 4.],\n",
       "        [1., 1., 2., 3., 4.],\n",
       "        [2., 2., 2., 3., 4.],\n",
       "        [3., 3., 3., 3., 3.]]),\n",
       " 3.0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OldLD('ab ', 'xyz ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24de251c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 2., 3., 4.],\n",
       "        [1., 1., 2., 3., 4.],\n",
       "        [2., 2., 2., 3., 4.],\n",
       "        [3., 2., 3., 3., 4.],\n",
       "        [4., 3., 3., 4., 3.]]),\n",
       " 3.0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OldLD('abxc', 'xyzc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
